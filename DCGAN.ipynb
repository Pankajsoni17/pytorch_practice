{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from scipy.misc import toimage\n",
    "from torch.utils import data as D\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = (64, 64, 1)\n",
    "latent_size = 100\n",
    "g_hidden_size = 128\n",
    "d_hidden_size = 64\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "dataset_dir = \"./data/FMNIST\"\n",
    "sample_dir = \"./result_dcgan_FMNIST/\"\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.Resize(img_size[0]),\n",
    "                #transforms.CenterCrop(img_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root=dataset_dir, train=True ,transform=transform, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size, kernel_size=4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # 1st deconv block\n",
    "            nn.ConvTranspose2d(latent_size, hidden_size * 8, kernel_size=kernel_size, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 2nd deconv block\n",
    "            nn.ConvTranspose2d(hidden_size * 8, hidden_size * 4, kernel_size=kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 3rd deconv block\n",
    "            nn.ConvTranspose2d(hidden_size * 4, hidden_size * 2, kernel_size=kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 4th deconv block\n",
    "            nn.ConvTranspose2d(hidden_size * 2, hidden_size * 1, kernel_size=kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 1),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 5th deconv block\n",
    "            nn.ConvTranspose2d(hidden_size * 1, input_size[2], kernel_size=kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # 64 x 64 x c --> 32 x 32 x hidden_size\n",
    "            nn.Conv2d(input_size[2], hidden_size, kernel_size=kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            # 32 x 32 x hidden_size --> 16 x 16 x hidden_size * 2\n",
    "            nn.Conv2d(hidden_size, hidden_size * 2, kernel_size=kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            # 16 x 16 x hidden_size * 2 --> 8 x 8 x hidden_size * 4\n",
    "            nn.Conv2d(hidden_size * 2, hidden_size * 4, kernel_size=kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            # 8 x 8 x hidden_size * 4 --> 4 x 4 x hidden_size * 8\n",
    "            nn.Conv2d(hidden_size * 4, hidden_size * 8, kernel_size=kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            # 4 x 4 x hidden_size * 8 --> 1 x 1 x 1\n",
    "            nn.Conv2d(hidden_size * 8, 1, kernel_size=kernel_size, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(img_size, latent_size, g_hidden_size, kernel_size=4)\n",
    "D = Discriminator(img_size, d_hidden_size, kernel_size=4)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "G.to(device)\n",
    "D.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee.hoseong\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1474: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Step [200/469], d_loss: 0.0112, g_loss: 7.0610, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [0/100], Step [400/469], d_loss: 0.0618, g_loss: 6.8804, D(x): 0.95, D(G(z)): 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee.hoseong\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1474: UserWarning: Using a target size (torch.Size([96, 1])) that is different to the input size (torch.Size([96, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [200/469], d_loss: 0.0326, g_loss: 7.7574, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [1/100], Step [400/469], d_loss: 0.0615, g_loss: 6.6831, D(x): 0.99, D(G(z)): 0.04\n",
      "Epoch [2/100], Step [200/469], d_loss: 0.0186, g_loss: 4.3121, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [2/100], Step [400/469], d_loss: 0.0591, g_loss: 8.6713, D(x): 0.95, D(G(z)): 0.00\n",
      "Epoch [3/100], Step [200/469], d_loss: 0.0634, g_loss: 7.2436, D(x): 1.00, D(G(z)): 0.05\n",
      "Epoch [3/100], Step [400/469], d_loss: 0.0804, g_loss: 6.5258, D(x): 0.94, D(G(z)): 0.01\n",
      "Epoch [4/100], Step [200/469], d_loss: 0.1907, g_loss: 2.7839, D(x): 0.87, D(G(z)): 0.01\n",
      "Epoch [4/100], Step [400/469], d_loss: 0.1269, g_loss: 6.3250, D(x): 0.91, D(G(z)): 0.02\n",
      "Epoch [5/100], Step [200/469], d_loss: 0.0472, g_loss: 5.9886, D(x): 0.98, D(G(z)): 0.03\n",
      "Epoch [5/100], Step [400/469], d_loss: 0.3958, g_loss: 11.5125, D(x): 0.77, D(G(z)): 0.00\n",
      "Epoch [6/100], Step [200/469], d_loss: 0.6871, g_loss: 6.6929, D(x): 0.62, D(G(z)): 0.00\n",
      "Epoch [6/100], Step [400/469], d_loss: 0.0171, g_loss: 8.6905, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [7/100], Step [200/469], d_loss: 0.0114, g_loss: 6.7488, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [7/100], Step [400/469], d_loss: 0.0811, g_loss: 5.2472, D(x): 0.94, D(G(z)): 0.01\n",
      "Epoch [8/100], Step [200/469], d_loss: 0.0321, g_loss: 7.6572, D(x): 0.99, D(G(z)): 0.02\n",
      "Epoch [8/100], Step [400/469], d_loss: 0.0208, g_loss: 6.1280, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [9/100], Step [200/469], d_loss: 0.0027, g_loss: 11.1519, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [9/100], Step [400/469], d_loss: 0.0744, g_loss: 6.4331, D(x): 1.00, D(G(z)): 0.06\n",
      "Epoch [10/100], Step [200/469], d_loss: 0.0169, g_loss: 5.0602, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [10/100], Step [400/469], d_loss: 0.0243, g_loss: 7.2650, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [11/100], Step [200/469], d_loss: 0.0214, g_loss: 11.3007, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [11/100], Step [400/469], d_loss: 0.0076, g_loss: 6.7518, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [12/100], Step [200/469], d_loss: 0.0874, g_loss: 8.5153, D(x): 0.93, D(G(z)): 0.00\n",
      "Epoch [12/100], Step [400/469], d_loss: 0.0228, g_loss: 4.7453, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [13/100], Step [200/469], d_loss: 0.1208, g_loss: 9.2082, D(x): 0.90, D(G(z)): 0.00\n",
      "Epoch [13/100], Step [400/469], d_loss: 0.0609, g_loss: 5.6012, D(x): 0.95, D(G(z)): 0.00\n",
      "Epoch [14/100], Step [200/469], d_loss: 0.1511, g_loss: 2.4625, D(x): 1.00, D(G(z)): 0.12\n",
      "Epoch [14/100], Step [400/469], d_loss: 0.0004, g_loss: 9.9415, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [15/100], Step [200/469], d_loss: 0.0093, g_loss: 9.0139, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [15/100], Step [400/469], d_loss: 0.0425, g_loss: 8.0740, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [16/100], Step [200/469], d_loss: 0.0067, g_loss: 8.6525, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [16/100], Step [400/469], d_loss: 0.0161, g_loss: 6.3399, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [17/100], Step [200/469], d_loss: 0.0358, g_loss: 6.7407, D(x): 0.99, D(G(z)): 0.03\n",
      "Epoch [17/100], Step [400/469], d_loss: 0.0240, g_loss: 9.1212, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [18/100], Step [200/469], d_loss: 0.6015, g_loss: 10.8922, D(x): 0.72, D(G(z)): 0.00\n",
      "Epoch [18/100], Step [400/469], d_loss: 1.3266, g_loss: 9.0157, D(x): 1.00, D(G(z)): 0.59\n",
      "Epoch [19/100], Step [200/469], d_loss: 0.0428, g_loss: 11.0721, D(x): 0.96, D(G(z)): 0.00\n",
      "Epoch [19/100], Step [400/469], d_loss: 0.0410, g_loss: 7.8634, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [20/100], Step [200/469], d_loss: 0.0280, g_loss: 6.7315, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [20/100], Step [400/469], d_loss: 0.0903, g_loss: 7.0590, D(x): 0.98, D(G(z)): 0.06\n",
      "Epoch [21/100], Step [200/469], d_loss: 0.0549, g_loss: 4.7113, D(x): 1.00, D(G(z)): 0.05\n",
      "Epoch [21/100], Step [400/469], d_loss: 0.2700, g_loss: 8.4264, D(x): 0.84, D(G(z)): 0.00\n",
      "Epoch [22/100], Step [200/469], d_loss: 0.0170, g_loss: 7.0597, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [22/100], Step [400/469], d_loss: 0.2903, g_loss: 8.4704, D(x): 0.97, D(G(z)): 0.18\n",
      "Epoch [23/100], Step [200/469], d_loss: 0.0818, g_loss: 5.1417, D(x): 0.94, D(G(z)): 0.01\n",
      "Epoch [23/100], Step [400/469], d_loss: 0.0224, g_loss: 7.7728, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [24/100], Step [200/469], d_loss: 0.1543, g_loss: 5.1906, D(x): 0.98, D(G(z)): 0.09\n",
      "Epoch [24/100], Step [400/469], d_loss: 0.0219, g_loss: 6.6149, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [25/100], Step [200/469], d_loss: 0.0318, g_loss: 8.5296, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [25/100], Step [400/469], d_loss: 1.2456, g_loss: 6.1861, D(x): 0.68, D(G(z)): 0.24\n",
      "Epoch [26/100], Step [200/469], d_loss: 0.0622, g_loss: 6.1292, D(x): 0.95, D(G(z)): 0.00\n",
      "Epoch [26/100], Step [400/469], d_loss: 0.0321, g_loss: 4.4032, D(x): 0.99, D(G(z)): 0.02\n",
      "Epoch [27/100], Step [200/469], d_loss: 0.1016, g_loss: 6.4755, D(x): 0.93, D(G(z)): 0.01\n",
      "Epoch [27/100], Step [400/469], d_loss: 0.0352, g_loss: 4.6568, D(x): 0.99, D(G(z)): 0.02\n",
      "Epoch [28/100], Step [200/469], d_loss: 0.0426, g_loss: 5.6491, D(x): 0.97, D(G(z)): 0.01\n",
      "Epoch [28/100], Step [400/469], d_loss: 0.0267, g_loss: 8.1586, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [29/100], Step [200/469], d_loss: 0.0014, g_loss: 14.8631, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [29/100], Step [400/469], d_loss: 0.0245, g_loss: 5.0719, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [30/100], Step [200/469], d_loss: 0.1355, g_loss: 3.6871, D(x): 0.99, D(G(z)): 0.10\n",
      "Epoch [30/100], Step [400/469], d_loss: 0.0226, g_loss: 8.4565, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [31/100], Step [200/469], d_loss: 0.0714, g_loss: 8.0878, D(x): 0.97, D(G(z)): 0.04\n",
      "Epoch [31/100], Step [400/469], d_loss: 0.4555, g_loss: 7.2697, D(x): 0.76, D(G(z)): 0.01\n",
      "Epoch [32/100], Step [200/469], d_loss: 0.0270, g_loss: 8.1603, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [32/100], Step [400/469], d_loss: 0.0252, g_loss: 5.7805, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [33/100], Step [200/469], d_loss: 0.0141, g_loss: 5.7633, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [33/100], Step [400/469], d_loss: 0.0440, g_loss: 5.4095, D(x): 1.00, D(G(z)): 0.04\n",
      "Epoch [34/100], Step [200/469], d_loss: 0.0107, g_loss: 8.2110, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [34/100], Step [400/469], d_loss: 0.0235, g_loss: 7.5889, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [35/100], Step [200/469], d_loss: 0.0225, g_loss: 8.0355, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [35/100], Step [400/469], d_loss: 0.0270, g_loss: 7.3076, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [36/100], Step [200/469], d_loss: 0.0168, g_loss: 8.4836, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [36/100], Step [400/469], d_loss: 0.0154, g_loss: 6.6030, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [37/100], Step [200/469], d_loss: 0.1532, g_loss: 7.7404, D(x): 0.96, D(G(z)): 0.07\n",
      "Epoch [37/100], Step [400/469], d_loss: 0.0430, g_loss: 6.5733, D(x): 0.98, D(G(z)): 0.02\n",
      "Epoch [38/100], Step [200/469], d_loss: 0.9492, g_loss: 6.9819, D(x): 0.56, D(G(z)): 0.00\n",
      "Epoch [38/100], Step [400/469], d_loss: 0.0098, g_loss: 2.5293, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [39/100], Step [200/469], d_loss: 0.3284, g_loss: 5.5044, D(x): 0.79, D(G(z)): 0.00\n",
      "Epoch [39/100], Step [400/469], d_loss: 0.0324, g_loss: 7.9236, D(x): 0.97, D(G(z)): 0.00\n",
      "Epoch [40/100], Step [200/469], d_loss: 0.0568, g_loss: 5.9074, D(x): 0.99, D(G(z)): 0.04\n",
      "Epoch [40/100], Step [400/469], d_loss: 0.0348, g_loss: 4.3334, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [41/100], Step [200/469], d_loss: 0.0324, g_loss: 6.9080, D(x): 0.99, D(G(z)): 0.02\n",
      "Epoch [41/100], Step [400/469], d_loss: 0.0080, g_loss: 6.2349, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [42/100], Step [200/469], d_loss: 0.5998, g_loss: 6.4414, D(x): 0.67, D(G(z)): 0.00\n",
      "Epoch [42/100], Step [400/469], d_loss: 0.0410, g_loss: 4.8442, D(x): 0.99, D(G(z)): 0.03\n",
      "Epoch [43/100], Step [200/469], d_loss: 0.0287, g_loss: 5.6113, D(x): 0.99, D(G(z)): 0.02\n",
      "Epoch [43/100], Step [400/469], d_loss: 0.0106, g_loss: 5.4241, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [44/100], Step [200/469], d_loss: 0.0520, g_loss: 6.2481, D(x): 1.00, D(G(z)): 0.05\n",
      "Epoch [44/100], Step [400/469], d_loss: 0.0300, g_loss: 8.3153, D(x): 1.00, D(G(z)): 0.03\n",
      "Epoch [45/100], Step [200/469], d_loss: 0.0167, g_loss: 8.1604, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [45/100], Step [400/469], d_loss: 0.0613, g_loss: 12.5171, D(x): 0.96, D(G(z)): 0.00\n",
      "Epoch [46/100], Step [200/469], d_loss: 0.1571, g_loss: 6.6142, D(x): 1.00, D(G(z)): 0.11\n",
      "Epoch [46/100], Step [400/469], d_loss: 0.0411, g_loss: 7.5134, D(x): 0.96, D(G(z)): 0.00\n",
      "Epoch [47/100], Step [200/469], d_loss: 0.0192, g_loss: 7.3999, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [47/100], Step [400/469], d_loss: 0.0959, g_loss: 7.5752, D(x): 0.94, D(G(z)): 0.01\n",
      "Epoch [48/100], Step [200/469], d_loss: 0.0437, g_loss: 7.4880, D(x): 1.00, D(G(z)): 0.04\n",
      "Epoch [48/100], Step [400/469], d_loss: 0.0072, g_loss: 8.9442, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [49/100], Step [200/469], d_loss: 0.0271, g_loss: 5.0133, D(x): 1.00, D(G(z)): 0.03\n",
      "Epoch [49/100], Step [400/469], d_loss: 0.1150, g_loss: 10.9549, D(x): 0.98, D(G(z)): 0.05\n",
      "Epoch [50/100], Step [200/469], d_loss: 0.0206, g_loss: 5.7676, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [50/100], Step [400/469], d_loss: 0.0615, g_loss: 6.2589, D(x): 0.96, D(G(z)): 0.02\n",
      "Epoch [51/100], Step [200/469], d_loss: 0.0126, g_loss: 11.6787, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [51/100], Step [400/469], d_loss: 0.0527, g_loss: 5.0848, D(x): 1.00, D(G(z)): 0.05\n",
      "Epoch [52/100], Step [200/469], d_loss: 0.0439, g_loss: 4.8618, D(x): 0.99, D(G(z)): 0.03\n",
      "Epoch [52/100], Step [400/469], d_loss: 0.0152, g_loss: 7.2044, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [53/100], Step [200/469], d_loss: 0.0240, g_loss: 6.1530, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [53/100], Step [400/469], d_loss: 0.0736, g_loss: 10.4885, D(x): 1.00, D(G(z)): 0.05\n",
      "Epoch [54/100], Step [200/469], d_loss: 0.4284, g_loss: 7.2380, D(x): 1.00, D(G(z)): 0.28\n",
      "Epoch [54/100], Step [400/469], d_loss: 0.0249, g_loss: 6.2288, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [55/100], Step [200/469], d_loss: 0.0060, g_loss: 6.2649, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [55/100], Step [400/469], d_loss: 0.3825, g_loss: 7.5493, D(x): 0.95, D(G(z)): 0.15\n",
      "Epoch [56/100], Step [200/469], d_loss: 0.0197, g_loss: 7.1056, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [56/100], Step [400/469], d_loss: 0.0098, g_loss: 8.2446, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [57/100], Step [200/469], d_loss: 0.0070, g_loss: 5.4367, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [57/100], Step [400/469], d_loss: 0.0033, g_loss: 6.6089, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [58/100], Step [200/469], d_loss: 0.0091, g_loss: 6.0396, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [58/100], Step [400/469], d_loss: 0.8607, g_loss: 8.2800, D(x): 1.00, D(G(z)): 0.39\n",
      "Epoch [59/100], Step [200/469], d_loss: 0.0052, g_loss: 7.3975, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [59/100], Step [400/469], d_loss: 0.0307, g_loss: 6.9870, D(x): 0.97, D(G(z)): 0.00\n",
      "Epoch [60/100], Step [200/469], d_loss: 0.0273, g_loss: 22.5141, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [60/100], Step [400/469], d_loss: 0.0110, g_loss: 7.4018, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [61/100], Step [200/469], d_loss: 0.0169, g_loss: 6.6619, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [61/100], Step [400/469], d_loss: 0.0101, g_loss: 6.0998, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [62/100], Step [200/469], d_loss: 0.0552, g_loss: 5.7051, D(x): 0.99, D(G(z)): 0.04\n",
      "Epoch [62/100], Step [400/469], d_loss: 0.0490, g_loss: 6.9460, D(x): 0.96, D(G(z)): 0.00\n",
      "Epoch [63/100], Step [200/469], d_loss: 0.0262, g_loss: 4.7287, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [63/100], Step [400/469], d_loss: 0.0167, g_loss: 6.1538, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [64/100], Step [200/469], d_loss: 0.0089, g_loss: 4.0970, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [64/100], Step [400/469], d_loss: 0.2819, g_loss: 4.3864, D(x): 1.00, D(G(z)): 0.16\n",
      "Epoch [65/100], Step [200/469], d_loss: 0.1669, g_loss: 6.4225, D(x): 0.88, D(G(z)): 0.01\n",
      "Epoch [65/100], Step [400/469], d_loss: 0.0220, g_loss: 6.2389, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [66/100], Step [200/469], d_loss: 0.0068, g_loss: 4.4221, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [66/100], Step [400/469], d_loss: 0.0239, g_loss: 13.6364, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [67/100], Step [200/469], d_loss: 0.0393, g_loss: 9.7194, D(x): 0.97, D(G(z)): 0.00\n",
      "Epoch [67/100], Step [400/469], d_loss: 0.0202, g_loss: 8.0491, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [68/100], Step [200/469], d_loss: 0.0854, g_loss: 7.6196, D(x): 0.94, D(G(z)): 0.00\n",
      "Epoch [68/100], Step [400/469], d_loss: 0.0158, g_loss: 6.3246, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [69/100], Step [200/469], d_loss: 0.0237, g_loss: 13.4247, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [69/100], Step [400/469], d_loss: 0.0666, g_loss: 5.2758, D(x): 0.97, D(G(z)): 0.03\n",
      "Epoch [70/100], Step [200/469], d_loss: 0.0186, g_loss: 5.5583, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [70/100], Step [400/469], d_loss: 0.0182, g_loss: 5.5443, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [71/100], Step [200/469], d_loss: 0.2189, g_loss: 10.5772, D(x): 0.85, D(G(z)): 0.00\n",
      "Epoch [71/100], Step [400/469], d_loss: 0.0458, g_loss: 13.5245, D(x): 0.97, D(G(z)): 0.00\n",
      "Epoch [72/100], Step [200/469], d_loss: 0.0116, g_loss: 5.8604, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [72/100], Step [400/469], d_loss: 0.0109, g_loss: 6.3413, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [73/100], Step [200/469], d_loss: 0.0056, g_loss: 8.6551, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [73/100], Step [400/469], d_loss: 0.0701, g_loss: 5.0151, D(x): 0.95, D(G(z)): 0.01\n",
      "Epoch [74/100], Step [200/469], d_loss: 0.4732, g_loss: 5.2569, D(x): 1.00, D(G(z)): 0.26\n",
      "Epoch [74/100], Step [400/469], d_loss: 0.7743, g_loss: 4.6973, D(x): 0.99, D(G(z)): 0.39\n",
      "Epoch [75/100], Step [200/469], d_loss: 0.0294, g_loss: 4.1375, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [75/100], Step [400/469], d_loss: 0.0076, g_loss: 4.1317, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [76/100], Step [200/469], d_loss: 0.0292, g_loss: 4.9773, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [76/100], Step [400/469], d_loss: 0.0201, g_loss: 6.3848, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [77/100], Step [200/469], d_loss: 0.0091, g_loss: 5.6035, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [77/100], Step [400/469], d_loss: 0.0458, g_loss: 3.9163, D(x): 1.00, D(G(z)): 0.04\n",
      "Epoch [78/100], Step [200/469], d_loss: 0.0107, g_loss: 5.7028, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [78/100], Step [400/469], d_loss: 0.0086, g_loss: 7.6228, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [79/100], Step [200/469], d_loss: 0.0539, g_loss: 2.9279, D(x): 0.98, D(G(z)): 0.02\n",
      "Epoch [79/100], Step [400/469], d_loss: 0.1389, g_loss: 5.1212, D(x): 0.92, D(G(z)): 0.03\n",
      "Epoch [80/100], Step [200/469], d_loss: 0.0073, g_loss: 9.9343, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [80/100], Step [400/469], d_loss: 0.0012, g_loss: 7.2356, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [81/100], Step [200/469], d_loss: 0.0098, g_loss: 6.0945, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [81/100], Step [400/469], d_loss: 0.0145, g_loss: 4.5618, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [82/100], Step [200/469], d_loss: 0.0398, g_loss: 5.8465, D(x): 0.97, D(G(z)): 0.01\n",
      "Epoch [82/100], Step [400/469], d_loss: 0.0111, g_loss: 6.6548, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [83/100], Step [200/469], d_loss: 0.0178, g_loss: 8.0563, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [83/100], Step [400/469], d_loss: 1.4433, g_loss: 7.3926, D(x): 0.96, D(G(z)): 0.57\n",
      "Epoch [84/100], Step [200/469], d_loss: 0.8181, g_loss: 6.6847, D(x): 0.59, D(G(z)): 0.00\n",
      "Epoch [84/100], Step [400/469], d_loss: 0.0049, g_loss: 6.8690, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [85/100], Step [200/469], d_loss: 0.0795, g_loss: 9.1003, D(x): 0.93, D(G(z)): 0.00\n",
      "Epoch [85/100], Step [400/469], d_loss: 0.1710, g_loss: 10.2571, D(x): 0.87, D(G(z)): 0.00\n",
      "Epoch [86/100], Step [200/469], d_loss: 0.0034, g_loss: 11.3746, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [86/100], Step [400/469], d_loss: 0.0454, g_loss: 8.7156, D(x): 0.97, D(G(z)): 0.00\n",
      "Epoch [87/100], Step [200/469], d_loss: 0.0366, g_loss: 8.0073, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [87/100], Step [400/469], d_loss: 0.0077, g_loss: 8.0440, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [88/100], Step [200/469], d_loss: 0.0196, g_loss: 5.7523, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [88/100], Step [400/469], d_loss: 0.0077, g_loss: 6.6046, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [89/100], Step [200/469], d_loss: 0.0374, g_loss: 4.2321, D(x): 1.00, D(G(z)): 0.03\n",
      "Epoch [89/100], Step [400/469], d_loss: 0.0146, g_loss: 4.2014, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [90/100], Step [200/469], d_loss: 0.0108, g_loss: 6.2388, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [90/100], Step [400/469], d_loss: 0.0071, g_loss: 5.9324, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [91/100], Step [200/469], d_loss: 0.0308, g_loss: 5.6333, D(x): 1.00, D(G(z)): 0.03\n",
      "Epoch [91/100], Step [400/469], d_loss: 0.0203, g_loss: 5.6262, D(x): 0.98, D(G(z)): 0.00\n",
      "Epoch [92/100], Step [200/469], d_loss: 0.0055, g_loss: 6.5936, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [92/100], Step [400/469], d_loss: 0.0104, g_loss: 8.6176, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [93/100], Step [200/469], d_loss: 0.0121, g_loss: 5.1591, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [93/100], Step [400/469], d_loss: 0.2749, g_loss: 2.3165, D(x): 0.98, D(G(z)): 0.12\n",
      "Epoch [94/100], Step [200/469], d_loss: 0.0355, g_loss: 5.7719, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [94/100], Step [400/469], d_loss: 0.0144, g_loss: 9.4160, D(x): 1.00, D(G(z)): 0.01\n",
      "Epoch [95/100], Step [200/469], d_loss: 0.0043, g_loss: 6.1383, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [95/100], Step [400/469], d_loss: 0.0463, g_loss: 5.0196, D(x): 1.00, D(G(z)): 0.04\n",
      "Epoch [96/100], Step [200/469], d_loss: 0.0325, g_loss: 6.0451, D(x): 0.97, D(G(z)): 0.00\n",
      "Epoch [96/100], Step [400/469], d_loss: 0.0305, g_loss: 5.2636, D(x): 1.00, D(G(z)): 0.03\n",
      "Epoch [97/100], Step [200/469], d_loss: 0.0080, g_loss: 8.7180, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [97/100], Step [400/469], d_loss: 0.8805, g_loss: 7.8793, D(x): 1.00, D(G(z)): 0.40\n",
      "Epoch [98/100], Step [200/469], d_loss: 0.0126, g_loss: 12.6686, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [98/100], Step [400/469], d_loss: 0.0682, g_loss: 8.5812, D(x): 1.00, D(G(z)): 0.06\n",
      "Epoch [99/100], Step [200/469], d_loss: 0.0020, g_loss: 8.1459, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [99/100], Step [400/469], d_loss: 0.0120, g_loss: 7.1635, D(x): 1.00, D(G(z)): 0.01\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "total_step = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(trainloader):\n",
    "        images = images.to(device)\n",
    "        #print(\"images shape : \", images.size())\n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(images.size()[0], 1).to(device)\n",
    "        fake_labels = torch.zeros(images.size()[0], 1).to(device)\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                        Train the discriminator                     #\n",
    "        # ================================================================== #\n",
    "\n",
    "        outputs = D(images)\n",
    "        \n",
    "        #print(\"outputs : \", outputs.shape)\n",
    "        #print(\"real labels : \", real_labels.shape)\n",
    "        \n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        z = torch.randn((images.size()[0], latent_size)).view(-1, latent_size, 1, 1).to(device)\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        #print(\"fake_images size : \", fake_images.shape)\n",
    "        \n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                        Train the generator                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        z = torch.randn((images.size()[0], latent_size)).view(-1, latent_size, 1, 1).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
    "                          real_score.mean().item(), fake_score.mean().item()))\n",
    "    \n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.reshape(images.size()[0], 1, 64, 64)\n",
    "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.reshape(fake_images.size()[0], 1, 64, 64)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "\n",
    "# Save the model checkpoints \n",
    "torch.save(G.state_dict(), sample_dir + '/G.ckpt')\n",
    "torch.save(D.state_dict(), sample_dir + '/D.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
