{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from scipy.misc import toimage\n",
    "from torch.utils import data as D\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_2d = True\n",
    "\n",
    "img_size = (28, 28, 1) if is_2d else 28*28\n",
    "hidden_size = 256\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "dataset_dir = \"./data/FMNIST\"\n",
    "sample_dir = \"./result_vanilla_ae_2d_FMNIST/\" if is_2d else \"./result_vanilla_ae_1d_FMNIST/\"\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if is_2d:\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize(img_size[0]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5),std=(0.5, 0.5, 0.5))])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5),std=(0.5, 0.5, 0.5))])\n",
    "    \n",
    "trainset = torchvision.datasets.FashionMNIST(root=dataset_dir, train=True ,transform=transform, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vanilla_autoencoder_1d(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Vanilla_autoencoder_1d, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            #784 --> 256\n",
    "            nn.Linear(input_size, hidden_size, bias=True),\n",
    "            #nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            #256 --> 64\n",
    "            nn.Linear(hidden_size, hidden_size//4, bias=True),\n",
    "            #nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            #64 --> 256\n",
    "            nn.Linear(hidden_size//4, hidden_size, bias=True),\n",
    "            #nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Linear(hidden_size, input_size, bias=True),\n",
    "            #nn.BatchNorm2d(hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Vanilla_autoencoder_2d(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Vanilla_autoencoder_2d, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            # (-1, 1, 28, 28) --> (-1, 256, 14, 14)\n",
    "            nn.Conv2d(input_size[2], hidden_size, kernel_size=3, padding=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # (-1, 256, 14, 14) --> (-1, 1024, 7, 7)\n",
    "            nn.Conv2d(hidden_size, hidden_size*4, kernel_size=3, padding=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size*4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # (-1, 1024, 7, 7) --> (-1, 256, 14, 14)\n",
    "            nn.ConvTranspose2d(hidden_size*4, hidden_size, kernel_size=3, padding=1, stride=2, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # (-1, 256, 14, 14) --> (-1, 1, 28, 28)\n",
    "            nn.ConvTranspose2d(hidden_size, input_size[2], kernel_size=3, padding=1, stride=2, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(input_size[2]),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Vanilla_autoencoder_2d(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(256, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(1024, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(256, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE_model = Vanilla_autoencoder_2d(img_size, hidden_size) if is_2d else Vanilla_autoencoder_1d(img_size, hidden_size)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "AE_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(AE_model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Step [800/938], mse loss: 0.1742\n",
      "Epoch [1/100], Step [800/938], mse loss: 0.0813\n",
      "Epoch [2/100], Step [800/938], mse loss: 0.0359\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "total_step = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(trainloader):\n",
    "        if not is_2d:\n",
    "            n_features = np.prod(images.size()[1:])\n",
    "            images = images.view(-1, n_features)\n",
    "        images = images.to(device)\n",
    "                \n",
    "        outputs = AE_model(images)\n",
    "        \n",
    "        mse_loss = criterion(outputs, images)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mse_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 800 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], mse loss: {:.4f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, mse_loss.item()))\n",
    "    \n",
    "    # Save real images and output_images\n",
    "    images = images.reshape(images.size()[0], 1, 28, 28)\n",
    "    output_images = outputs.reshape(outputs.size()[0], 1, 28, 28)\n",
    "        \n",
    "    results = torch.cat([images, output_images], dim=2)\n",
    "    \n",
    "    save_image(denorm(results), os.path.join(sample_dir, 'ae_model_result-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
